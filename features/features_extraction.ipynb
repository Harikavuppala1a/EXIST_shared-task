{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweet = \"I have read the Bible and you are right. The letters of St Paul are supporting the oppression of women (to give one example out of many). Also, if being swallowed by a large fish and coming out of it alive is natural law, then I would travel by fish and not by boat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "st1 = \"I need someone to make me cum my pussy is throbbing for some cock dm me for some fun ;)#horny #dm #sext #sexting #trade #tribute #cumtribute #pussy #wet #orgasm #cumslut #slut #sex #fuckme #dirtytalk #verbal #porn #cock #cum #dirty #hornyfemale #wetpussy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I need someone to make me cum my pussy is throbbing for some cock dm me for some fun ;)#horny #dm #sext #sexting #trade #tribute #cumtribute #pussy #wet #orgasm #cumslut #slut #sex #fuckme #dirtytalk #verbal #porn #cock #cum #dirty #hornyfemale #wetpussy'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import preprocessor as p\n",
    "st1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install empath\n",
    "from empath import Empath\n",
    "lexicon = Empath()    \n",
    "# lexicon.create_category(\"sexism\",[\"abuse\", \"violence\", \"oppression\", \"stereotype\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def empath_features(text):\n",
    "    return lexicon.analyze(text,categories = ['sexism','violence', 'money', 'valuable', 'domestic_work', 'hate', 'aggression', 'anticipation', 'crime', 'weakness', 'horror', 'swearing_terms', 'kill', 'sexual', 'cooking', 'exasperation', 'body', 'ridicule', 'disgust', 'anger', 'rage']\n",
    ", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sexism': 0.0,\n",
       " 'violence': 0.0,\n",
       " 'money': 0.0,\n",
       " 'valuable': 0.0,\n",
       " 'domestic_work': 0.0,\n",
       " 'hate': 0.0,\n",
       " 'aggression': 0.0,\n",
       " 'anticipation': 0.024390243902439025,\n",
       " 'crime': 0.0,\n",
       " 'weakness': 0.0,\n",
       " 'horror': 0.0,\n",
       " 'swearing_terms': 0.0,\n",
       " 'kill': 0.0,\n",
       " 'sexual': 0.024390243902439025,\n",
       " 'cooking': 0.0,\n",
       " 'exasperation': 0.0,\n",
       " 'body': 0.0,\n",
       " 'ridicule': 0.0,\n",
       " 'disgust': 0.0,\n",
       " 'anger': 0.0,\n",
       " 'rage': 0.0}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empath_features(st1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tweet = 'Preprocessor is #awesome #damn ! üëç üëç üòÄüòëüí® :) https://github.ü§•com/s/preprocessor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install emoji --upgrade\n",
    "import emoji\n",
    "def emoji_text(text, language):\n",
    "    p.set_options(p.OPT.EMOJI)\n",
    "    parsed_tweet = p.parse(text)\n",
    "    emoji_list = []\n",
    "    for emot in parsed_tweet.emojis:\n",
    "        emot = emot.match\n",
    "        emoji_list.append(emoji.demojize(emot, language=language).replace('_', ' ').strip(':'))    \n",
    "    return emoji_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thumbs up',\n",
       " 'thumbs up',\n",
       " 'grinning face',\n",
       " 'expressionless face',\n",
       " 'dashing away']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_text(sample_tweet, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hashtag_features(text):\n",
    "    p.set_options(p.OPT.HASHTAG)\n",
    "    parsed_tweet = p.parse(text)\n",
    "    hashtag_list = []\n",
    "    for hg in parsed_tweet.hashtags:\n",
    "        hashtag_list.append(hg.match.lstrip('#'))    \n",
    "    return hashtag_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['awesome', 'damn']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hashtag_features(sample_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, requests, json\n",
    "api_key = 'AIzaSyBfAcfdHYFIYxCszLqn4AHwym4QXofB-eY'\n",
    "url = ('https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze' +'?key=' + api_key)\n",
    "en_attributes = ['TOXICITY', 'SEVERE_TOXICITY', 'TOXICITY_FAST', 'IDENTITY_ATTACK', 'INSULT', 'PROFANITY', 'THREAT', 'SEXUALLY_EXPLICIT', 'OBSCENE']\n",
    "es_attributes = ['TOXICITY', 'SEVERE_TOXICITY''TOXICITY_FAST_EXPERIMENTAL', 'IDENTITY_ATTACK_EXPERIMENTAL']\n",
    "en_attr_dict = {}\n",
    "for attr in en_attributes:\n",
    "    en_attr_dict[attr] = {}   \n",
    "es_attr_dict = {}\n",
    "for attr in es_attributes:\n",
    "    es_attr_dict[attr] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perspective_features(text, language):\n",
    "    if language == 'en':\n",
    "        attr_dict = en_attr_dict\n",
    "        attributes = en_attributes\n",
    "    else:\n",
    "        attr_dict = es_attr_dict\n",
    "        attributes = es_attributes\n",
    "    data_dict = {\n",
    "        'comment': {'text': text},\n",
    "        'languages': language,\n",
    "        'requestedAttributes': attr_dict\n",
    "    }\n",
    "    time.sleep(1.01)\n",
    "    response = requests.post(url=url, data=json.dumps(data_dict)) \n",
    "    response_dict = json.loads(response.content)\n",
    "    pers_dict = {\"summary\": {}, \"span\": {}}\n",
    "    for attr in attributes:\n",
    "        pers_dict[\"summary\"][attr] = response_dict[\"attributeScores\"][attr][\"summaryScore\"][\"value\"]\n",
    "        curr_span = []\n",
    "        spanScores = response_dict[\"attributeScores\"][attr][\"spanScores\"]\n",
    "        for span in spanScores:\n",
    "            curr_span.append({'begin': span['begin'], 'end': span['end'], 'score': span['score']['value']})\n",
    "        pers_dict[\"span\"][attr] = curr_span\n",
    "    \n",
    "    return pers_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': {'TOXICITY': 0.53490317,\n",
       "  'SEVERE_TOXICITY': 0.3107015,\n",
       "  'TOXICITY_FAST': 0.00020175106,\n",
       "  'IDENTITY_ATTACK': 0.14081103,\n",
       "  'INSULT': 0.45062864,\n",
       "  'PROFANITY': 0.783563,\n",
       "  'THREAT': 0.112508096,\n",
       "  'SEXUALLY_EXPLICIT': 0.056633443,\n",
       "  'OBSCENE': 0.94219655},\n",
       " 'span': {'TOXICITY': [{'begin': 0, 'end': 84, 'score': 0.53490317}],\n",
       "  'SEVERE_TOXICITY': [{'begin': 0, 'end': 84, 'score': 0.3107015}],\n",
       "  'TOXICITY_FAST': [{'begin': 0, 'end': 33, 'score': 0.059446827},\n",
       "   {'begin': 33, 'end': 84, 'score': 0}],\n",
       "  'IDENTITY_ATTACK': [{'begin': 0, 'end': 84, 'score': 0.14081103}],\n",
       "  'INSULT': [{'begin': 0, 'end': 84, 'score': 0.45062864}],\n",
       "  'PROFANITY': [{'begin': 0, 'end': 84, 'score': 0.783563}],\n",
       "  'THREAT': [{'begin': 0, 'end': 84, 'score': 0.112508096}],\n",
       "  'SEXUALLY_EXPLICIT': [{'begin': 0, 'end': 84, 'score': 0.056633443}],\n",
       "  'OBSCENE': [{'begin': 0, 'end': 84, 'score': 0.94219655}]}}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perspective_features(sample_tweet, 'en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.models as gsm\n",
    "e2v = gsm.KeyedVectors.load_word2vec_format('emoji2vec.bin', binary=True)\n",
    "\n",
    "def getEmojiEmbeddings(emojiList,dim=300,verbose = False):\n",
    "    \"\"\" Generates an emoji vector by averaging the emoji representation for each emoji. If no emoji returns an empty list of dimension dim\"\"\"\n",
    "    if dim < 300:\n",
    "        raise IndexError(\"Dim has to be greater than 300\")\n",
    "    result = np.zeros(dim)\n",
    "    if (len(emojiList) == 0):\n",
    "        return result\n",
    "    else:\n",
    "        embs = None\n",
    "        for i in emojiList:\n",
    "            if verbose:\n",
    "                if i not in e2v.vocab:\n",
    "                      print(i)\n",
    "    embs = np.mean([e2v[i] for i in emojiList if i in e2v.vocab], axis=0)\n",
    "    if np.any(np.isnan(embs)):\n",
    "        return result\n",
    "    result[:300] = embs\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Use this for getting the emoji2vec embeddings for a whole sentence\n",
    "def emoji_features(text):\n",
    "        p.set_options(p.OPT.EMOJI)\n",
    "        parsed_tweet = p.parse(text)\n",
    "        emoji_list = []\n",
    "        for emot in parsed_tweet.emojis:\n",
    "            emot = emot.match\n",
    "            emoji_list.append(emot)    \n",
    "        embs = np.asarray([getEmojiEmbeddings(i,verbose=True) for i in emoji_list])\n",
    "        return embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "st1 = 'Preprocessor is #awesome #damn ! üëçüí® :) https://github.ü§•com/s/preprocessor'\n",
    "st2 = 'Preprocessor is #awesome #damn ! üëç üëçüí® :) https://github.ü§•com/s/preprocessor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = emoji_features(st1)\n",
    "b = emoji_features(st2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
